# IA

Reposit√≥rio: [https://github.com/cmmvio/cmmv-ai](https://github.com/cmmvio/cmmv-ai)

O m√≥dulo **@cmmv/ai** oferece suporte completo para **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)** e, no futuro, **S√≠ntese Aumentada por Recupera√ß√£o (RAS)** para **LLMs**. Ele permite a compreens√£o e gera√ß√£o de c√≥digo, suportando:

* **Tokeniza√ß√£o e Mapeamento de C√≥digo** ‚Äì Extrai tokens estruturados de arquivos TypeScript/JavaScript.
* **Cria√ß√£o de Conjunto de Dados RAG** ‚Äì Gera conjuntos de dados bin√°rios para busca vetorial.
* **Busca Vetorial com FAISS e Bancos de Dados Vetoriais** ‚Äì Suporta **Qdrant, Milvus, Neo4j**.
* **Integra√ß√£o com Hugging Face** ‚Äì Usa `transformers` para embeddings.
* **Modelos de Embedding Personalizados** ‚Äì Suporta `WhereIsAI/UAE-Large-V1`, `MiniLM`, `CodeLlama`, `DeepSeek`, entre outros.
* **Integra√ß√£o com Bancos de Dados** ‚Äì Suporta `Elasticsearch`, `Pinecone`, `Qdrant`, `PGVector`, entre outros.
* **Integra√ß√£o com LLM** ‚Äì Suporta `OpenAI`, `Hugging Face`, `Ollama`, `DeepSeek`, `Groq`, `Gemini`, entre outros.

## Instalar Python

Antes de instalar a CLI do Hugging Face, certifique-se de que o **Python** est√° instalado no seu sistema.

Execute o seguinte comando para instalar o Python no Ubuntu:

```sh
$ sudo apt update && sudo apt install python3 python3-pip -y
```

Para outros sistemas operacionais, consulte a p√°gina oficial de [download do Python](https://www.python.org/downloads/).

## Instalar a CLI do Hugging Face

Com o Python instalado, instale a CLI do Hugging Face usando **pip**:

```sh
$ pip3 install -U "huggingface_hub[cli]"
```

## Garantir que a CLI seja Reconhecida

Se o terminal n√£o reconhecer `huggingface-cli`, adicione `~/.local/bin` ao seu **PATH** do sistema:

```sh
$ echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
$ source ~/.bashrc
```

Execute o seguinte comando para verificar a instala√ß√£o:

```sh
$ huggingface-cli --help
```

Se o comando funcionar, a instala√ß√£o foi bem-sucedida! üéâ

## Autenticar com Hugging Face

Para acessar e baixar modelos, voc√™ precisa se autenticar.

Execute:

```sh
$ huggingface-cli login
```

Voc√™ ser√° solicitado a inserir seu **token de acesso do Hugging Face**.
Gere um em: [Tokens do Hugging Face](https://huggingface.co/settings/tokens)
Certifique-se de que o token tenha permiss√µes de **LEITURA**.

## Baixar Modelos

Para baixar um modelo, use o seguinte comando:

```sh
$ huggingface-cli download meta-llama/CodeLlama-7B-Python-hf --local-dir ./models/CodeLlama-7B
```

Isso baixar√° o modelo **CodeLlama 7B Python** para o diret√≥rio `./models/CodeLlama-7B`.

Para o **CMMV**, defina o caminho do modelo no arquivo `.cmmv.config.cjs`:

```js
huggingface: {
    token: process.env.HUGGINGFACE_HUB_TOKEN,
    localModelPath: './models',
    allowRemoteModels: true
},
llm: {
    provider: "google",
    embeddingTopk: 10,
    modelName: "gemini-1.5-pro",
    textMaxTokens: 2048,
    apiKey: process.env.GOOGLE_API_KEY,
    language: 'pt-br'
}
```

Agora seu ambiente est√° configurado para usar modelos do Hugging Face com o **CMMV**! üöÄ

## Configura√ß√£o

Dada a complexidade deste m√≥dulo, v√°rias configura√ß√µes s√£o necess√°rias, especialmente para definir quais modelos usar em cada etapa. Abaixo est√° a **configura√ß√£o base** com todas as propriedades dispon√≠veis atualmente.

### **Exemplo de Configura√ß√£o (`.cmmv.config.cjs`)**
<br/>

```javascript
require('dotenv').config();

module.exports = {
    env: process.env.NODE_ENV,

    ai: {
        huggingface: {
            token: process.env.HUGGINGFACE_HUB_TOKEN,
            localModelPath: './models',
            allowRemoteModels: true
        },
        tokenizer: {
            provider: "huggingface",
            model: "sentence-transformers/distilbert-base-nli-mean-tokens",
            indexSize: 768,
            useKeyBERT: false,
            chunkSize: 1000,
            chunkOverlap: 0,
            patterns: [
                //'../cmmv/**/*.ts',
                //'../cmmv/src/**/*.ts',
                //'../cmmv/packages/**/*.ts',
                //'../cmmv-*/**/*.ts',
                //'../cmmv-*/src/*.ts',
                //'../cmmv-*/src/**/*.ts',
                //'../cmmv-*/packages/**/*.ts',
                '../cmmv-*/**/*.md',
                '../cmmv-docs/docs/en/**/*.md'
            ],
            output: "./samples/data.bin",
            ignore: [
                "node_modules", "*.d.ts", "*.cjs",
                "*.spec.ts", "*.test.ts", "/tools/gulp/"
            ],
            exclude: [
                "cmmv-formbuilder", "cmmv-ui",
                "cmmv-language-tools", "cmmv-vue",
                "cmmv-reactivity", "cmmv-vite-plugin",
                "eslint.config.ts", "vitest.config.ts",
                "auto-imports.d.ts", ".d.ts", ".cjs",
                ".spec.ts", ".test.ts", "/tools/gulp/",
                "node_modules"
            ]
        },
        vector: {
            provider: "neo4j",
            qdrant: {
                url: 'http://localhost:6333',
                collection: 'embeddings'
            },
            neo4j: {
                url: "bolt://localhost:7687",
                username: process.env.NEO4J_USERNAME,
                password: process.env.NEO4J_PASSWORD,
                indexName: "vector",
                keywordIndexName: "keyword",
                nodeLabel: "Chunk",
                embeddingNodeProperty: "embedding"
            }
        },
        llm: {
            provider: "google",
            embeddingTopk: 10,
            model: "gemini-1.5-pro",
            textMaxTokens: 2048,
            apiKey: process.env.GOOGLE_API_KEY,
            language: 'pt-br'
        }
    }
};
```

| **Caminho**                                   | **Descri√ß√£o**                                      | **Valor Padr√£o / Exemplo**                         |
|-----------------------------------------------|--------------------------------------------------|------------------------------------------------|
| `ai.huggingface.token`                        | Token da API para o Hugging Face Hub             | `process.env.HUGGINGFACE_HUB_TOKEN`            |
| `ai.huggingface.localModelPath`               | Caminho para modelos locais                      | `./models`                                     |
| `ai.huggingface.allowRemoteModels`            | Permite baixar modelos do Hugging Face Hub       | `true`                                         |
| `ai.tokenizer.provider`                       | Provedor de tokeniza√ß√£o                          | `"huggingface"`                                |
| `ai.tokenizer.model`                          | Modelo de tokeniza√ß√£o                            | `"sentence-transformers/distilbert-base-nli-mean-tokens"` |
| `ai.tokenizer.indexSize`                      | Tamanho do √≠ndice de embeddings                  | `768`                                          |
| `ai.tokenizer.useKeyBERT`                     | Habilita o KeyBERT para extra√ß√£o de palavras-chave | `false`                                        |
| `ai.tokenizer.chunkSize`                      | Tamanho dos peda√ßos de texto para processamento  | `1000`                                         |
| `ai.tokenizer.chunkOverlap`                   | Sobreposi√ß√£o entre peda√ßos de texto              | `0`                                            |
| `ai.tokenizer.patterns`                       | Padr√µes de busca de arquivos para tokeniza√ß√£o    | `['../cmmv-*/**/*.md', '../cmmv-docs/docs/en/**/*.md']` |
| `ai.tokenizer.output`                         | Arquivo de sa√≠da para dados tokenizados          | `"./samples/data.bin"`                         |
| `ai.tokenizer.ignore`                         | Padr√µes de arquivos a ignorar                    | `["node_modules", "*.d.ts", "*.cjs", "*.spec.ts", "*.test.ts", "/tools/gulp/"]` |
| `ai.tokenizer.exclude`                        | Arquivos e diret√≥rios a excluir                  | `["cmmv-formbuilder", "cmmv-ui", "cmmv-language-tools", "cmmv-vue", "cmmv-reactivity", "cmmv-vite-plugin", "eslint.config.ts", "vitest.config.ts", "auto-imports.d.ts", ".d.ts", ".cjs", ".spec.ts", ".test.ts", "/tools/gulp/", "node_modules"]` |
| `ai.vector.provider`                          | Provedor de armazenamento vetorial               | `"neo4j"`                                      |
| `ai.vector.qdrant.url`                        | URL do servi√ßo Qdrant                            | `"http://localhost:6333"`                      |
| `ai.vector.qdrant.collection`                 | Nome da cole√ß√£o para Qdrant                      | `"embeddings"`                                 |
| `ai.vector.neo4j.url`                         | URL do banco de dados Neo4j                      | `"bolt://localhost:7687"`                      |
| `ai.vector.neo4j.username`                    | Nome de usu√°rio do Neo4j                         | `process.env.NEO4J_USERNAME`                   |
| `ai.vector.neo4j.password`                    | Senha do Neo4j                                   | `process.env.NEO4J_PASSWORD`                   |
| `ai.vector.neo4j.indexName`                   | Nome do √≠ndice para armazenamento vetorial       | `"vector"`                                     |
| `ai.vector.neo4j.keywordIndexName`            | Nome do √≠ndice para busca por palavras-chave     | `"keyword"`                                    |
| `ai.vector.neo4j.nodeLabel`                   | R√≥tulo para n√≥s vetorizados                      | `"Chunk"`                                      |
| `ai.vector.neo4j.embeddingNodeProperty`       | Propriedade que armazena embeddings vetoriais    | `"embedding"`                                  |
| `ai.llm.provider`                             | Provedor de LLM                                  | `"google"`                                     |
| `ai.llm.embeddingTopk`                        | N√∫mero de resultados top-k para embeddings       | `10`                                           |
| `ai.llm.model`                                | Nome do modelo LLM                               | `"gemini-1.5-pro"`                             |
| `ai.llm.textMaxTokens`                        | M√°ximo de tokens por requisi√ß√£o                  | `2048`                                         |
| `ai.llm.apiKey`                               | Chave de API para o provedor de LLM              | `process.env.GOOGLE_API_KEY`                   |
| `ai.llm.language`                             | Idioma padr√£o                                    | `"pt-br"`                                      |

## Convers√£o de Modelos

Alguns **LLMs (Modelos de Linguagem de Grande Escala)** n√£o s√£o nativamente compat√≠veis com todos os frameworks de infer√™ncia. Um exemplo importante √© o **Gemma da Google**, que n√£o √© diretamente suportado por muitas ferramentas. Para usar esses modelos de forma eficiente, voc√™ precisa **convert√™-los para o formato ONNX**.

ONNX (**Open Neural Network Exchange**) √© um formato aberto que otimiza modelos para infer√™ncia eficiente em m√∫ltiplas plataformas. Muitos frameworks de infer√™ncia, como **ONNX Runtime**, **TensorRT** e **OpenVINO**, suportam ONNX para uma implanta√ß√£o mais r√°pida e escal√°vel.

Antes de converter, instale os pacotes necess√°rios:

```sh
$ pip3 install -U "optimum[exporters]" onnx onnxruntime
```

Para converter o modelo **Gemma 2B da Google**, execute:

```sh
$ python3 -m optimum.exporters.onnx --model google/gemma-2b ./models/gemma-2b-onnx
```

## Tokenizador

A classe **Tokenizer** √© respons√°vel por processar e indexar arquivos no pipeline RAG. Ela oferece v√°rias configura√ß√µes, incluindo:

* **Padr√µes de Busca de Arquivos (`patterns`)**: Define padr√µes glob para buscar arquivos.
* **Exclus√µes e Arquivos Ignorados (`ignore`, `exclude`)**: Filtra arquivos desnecess√°rios da indexa√ß√£o.
* **Modelo de Embeddings (`model`)**: Seleciona o modelo do Hugging Face para gerar embeddings.
* **Tamanho do √çndice (`indexSize`)**: Ajusta as dimens√µes do embedding com base no modelo selecionado.
* **Fragmenta√ß√£o (`chunkSize`, `chunkOverlap`)**: Controla a segmenta√ß√£o de dados para melhor recupera√ß√£o.
* **Gera√ß√£o de Metadados com KeyBERT (`useKeyBERT`)**: Extrai palavras-chave para aprimorar buscas vetoriais.

Para outros modelos que requerem autentica√ß√£o (por exemplo, `LLaMA`), forne√ßa a chave de API em `huggingface.token`.

### Exemplo de Uso:
<br/>

```typescript
import { Application, Hook, HooksType } from '@cmmv/core';

class TokenizerSample {
    @Hook(HooksType.onInitialize)
    async start() {
        const { Tokenizer } = await import('@cmmv/ai');
        const tokenizer = new Tokenizer();
        tokenizer.start();
    }
}

Application.exec({
    services: [TokenizerSample],
});
```
<br/>

1. **Escaneia os diret√≥rios do projeto** com base na configura√ß√£o `patterns`.
2. **Analisa arquivos TypeScript/JavaScript/Markdown**, extraindo **fun√ß√µes, classes, enums, interfaces, constantes e decoradores**.
3. **Gera embeddings** usando modelos do Hugging Face.
4. **Armazena o conjunto de dados** em um arquivo bin√°rio `.bin`.

## Modelos de Embedding Comuns

| **Embedding**   | **Modelo Padr√£o**                         | **Requer Chave de API** |
|----------------|-----------------------------------------|-----------------------|
| Bedrock       | amazon.titan-embed-text-v1             | Sim                   |
| Cohere        | embed-english-v3.0                     | N√£o                   |
| DeepInfra     | -                                       | Sim                   |
| Doubao        | -                                       | Sim                   |
| Fireworks     | nomic-ai/nomic-embed-text-v1.5         | Sim                   |
| HuggingFace   | Xenova/all-MiniLM-L6-v2                | N√£o                   |
| LlamaCpp      | - (requer arquivo de modelo local)     | N√£o                   |
| OpenAI        | text-embedding-3-large                 | Sim                   |
| Pinecone      | multilingual-e5-large                  | N√£o                   |
| Tongyi        | -                                       | Sim                   |
| Watsonx       | -                                       | Sim                   |
| Jina          | jina-clip-v2                            | Sim                   |
| MiniMax       | embo-01                                | N√£o                   |
| Premai        | -                                       | N√£o                   |
| Hunyuan       | -                                       | Sim                   |
| TensorFlow    | -                                       | N√£o                   |
| TogetherAI    | togethercomputer/m2-bert-80M-8k-retrieval | Sim                   |
| Voyage        | voyage-01                               | Sim                   |
| ZhipuAI       | embedding-2                            | Sim                   |

* *[https://huggingface.co/models?pipeline_tag=feature-extraction&library=transformers.js&sort=downloads](https://huggingface.co/models?pipeline_tag=feature-extraction&library=transformers.js&sort=downloads)*
* *[https://v03.api.js.langchain.com/index.html](https://v03.api.js.langchain.com/index.html)*

## Usando KeyBERT

O **KeyBERT** √© um recurso opcional que aprimora a indexa√ß√£o extraindo palavras-chave relevantes. Ele ajuda a refinar os resultados de busca em **FAISS** ou bancos de dados vetoriais, melhorando a precis√£o das **consultas LLM**.

Diferente de **TF-IDF**, **YAKE!** ou **RAKE**, que dependem de m√©todos estat√≠sticos, o **KeyBERT** utiliza **embeddings BERT** para gerar palavras-chave mais significativas. Isso resulta em uma filtragem de busca melhor, levando a **respostas baseadas em LLM mais precisas**.

Se o KeyBERT **n√£o estiver habilitado**, o m√©todo padr√£o de extra√ß√£o de palavras-chave ser√° **TF-IDF**, que pode n√£o ser t√£o preciso, mas √© significativamente mais r√°pido.

Antes de usar o KeyBERT, certifique-se de ter o **Python 3** instalado. Em seguida, instale o KeyBERT usando **pip**:

```bash
$ pip install keybert
```

Uma vez instalado, o KeyBERT ser√° usado **durante a tokeniza√ß√£o** para gerar **palavras-chave de filtragem**. Essas palavras-chave melhoram a classifica√ß√£o do conte√∫do indexado, tornando os resultados da busca baseada em vetores mais relevantes.

Se voc√™ prefere um **processamento mais r√°pido**, pode desabilitar o KeyBERT, e o sistema voltar√° a usar **TF-IDF**.

Para habilitar o **KeyBERT**, atualize seu arquivo `.cmmv.config.cjs`:

```javascript
module.exports = {
    ai: {
        tokenizer: {
            useKeyBERT: true // Defina como false para usar TF-IDF
        }
    }
};
```

Com o **KeyBERT habilitado**, a filtragem de busca se torna mais **consciente do contexto**, levando a **respostas LLM mais precisas**.

Para mais detalhes sobre o KeyBERT, visite: [Documenta√ß√£o do KeyBERT](https://github.com/MaartenGr/KeyBERT).

## Conjunto de Dados

A classe **Dataset** gerencia o **armazenamento vetorizado** para recupera√ß√£o r√°pida.

* Salva embeddings em **formato bin√°rio** (`.bin`).
* Busca em mem√≥ria baseada em **FAISS**.
* Suporte para **Neo4j, Elasticsearch, PgVector, Qdrant**.

```typescript
const dataset = new Dataset();
dataset.save(); // Salva o conjunto de dados em formato bin√°rio
dataset.load(); // Carrega o conjunto de dados na mem√≥ria
```

Para armazenar e buscar **embeddings** de forma eficiente com `@cmmv/ai`.

| Banco de Dados | C√≥digo Aberto | Suporte Node.js | Backend de Armazenamento | Busca por Similaridade               |
|---------------|--------------|----------------|-------------------------|------------------------------------|
| **Qdrant**    | ‚úÖ Sim       | ‚úÖ Sim (`@qdrant/js-client-rest`) | Disco/Mem√≥ria | Cosseno, Euclidiana, Produto Escalar |
| **Milvus**    | ‚úÖ Sim       | ‚úÖ Sim (`@zilliz/milvus2-sdk-node`) | Disco/Mem√≥ria | IVF_FLAT, HNSW, PQ                |
| **Neo4j**     | ‚úÖ Sim (Comunidade) | ‚úÖ Sim (`neo4j-driver`) | Banco de Dados de Grafos | Busca vetorial baseada em Cypher   |
| **Elasticsearch** | ‚úÖ Sim     | ‚úÖ Sim (`@elastic/elasticsearch`) | Disco | k-NN, Vizinhos Mais Pr√≥ximos Aproximados (ANN) |
| **PGVector**  | ‚úÖ Sim       | ‚úÖ Sim (`pg`) | PostgreSQL | Cosseno, Euclidiana, Produto Interno |

Para executar esses bancos de dados localmente, use os seguintes
**comandos Docker**:

### **üîπ Qdrant**
```bash
$ docker run -p 6333:6333 --name qdrant-server qdrant/qdrant
```
<br/>

* Executa um servidor **Qdrant** na porta `6333`.
* API dispon√≠vel em `http://localhost:6333`.

### **üîπ Milvus**
```bash
$ docker run -p 19530:19530 --name milvus-server milvusdb/milvus
```
<br/>

* Executa **Milvus** na porta `19530`.
* Requer **SDK Python/Node** para intera√ß√£o.

### **üîπ Neo4j**
```bash
$ docker run --publish=7474:7474 --publish=7687:7687 --volume=$HOME/neo4j/data:/data --name neo4j-server neo4j
```
<br/>

* Executa **Neo4j** nas portas `7474` (HTTP) e `7687` (Bolt).
* Os dados s√£o armazenados persistentemente em `$HOME/neo4j/data`.

### **üîπ PGVector**
```bash
docker run --name pgvector-db -e POSTGRES_USER=admin -e POSTGRES_PASSWORD=admin -e POSTGRES_DB=vector_db -p 5432:5432 -d ankane/pgvector
```
* Executa **PostgreSQL** com PGVector na porta `5432`.
* Banco de dados padr√£o √© `vector_db` com usu√°rio `admin` e senha `admin`.

### **üîπ Elasticsearch**
```bash
docker run -d --name elasticsearch -p 9200:9200 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:8.5.1
```
* Executa **Elasticsearch** na porta `9200`.
* Modo de n√≥ √∫nico habilitado para uso local.

## LLMs

O m√≥dulo @cmmv/ai inclui suporte para m√∫ltiplos LLMs (Modelos de Linguagem de Grande Escala), permitindo integra√ß√£o flex√≠vel com diferentes provedores. Atualmente, os seguintes modelos s√£o suportados:

* **DeepSeek** ‚Äì Otimizado para programa√ß√£o e pesquisa t√©cnica.
* **Gemini (Google)** ‚Äì Um LLM multimodal com capacidades avan√ßadas de racioc√≠nio.
* **Hugging Face** ‚Äì Compat√≠vel com modelos de c√≥digo aberto como CodeLlama, MiniLM, DeepSeek e outros.
* **OpenAI (ChatGPT)** ‚Äì Integra√ß√£o com modelos como GPT-4 e GPT-3.5.
* **Ollama (Facebook)** ‚Äì Execu√ß√£o de modelos locais para aplica√ß√µes focadas em privacidade.
* **Groq (X)** ‚Äì Infer√™ncia de alta velocidade com modelos LLama-3, Mixtral e Gemma.

| **Provedor LLM**  | **Modelo Padr√£o**                      | **Requer Chave de API** |
|-------------------|--------------------------------------|-----------------------|
| **AI21 Labs**     | `j1-jumbo`, `j1-large`               | Sim                   |
| **Aleph Alpha**   | `luminous-base`, `luminous-extended` | Sim                   |
| **Anthropic**     | `claude-3-haiku-20240307`           | Sim                   |
| **AWS Bedrock**   | V√°rios modelos (Claude, Mistral, etc.) | Sim             |
| **Cohere**        | `command-xlarge-nightly`, `command-medium` | Sim        |
| **DeepInfra**     | V√°rios modelos                      | Sim                   |
| **DeepSeek**      | `deepseek-ai/deepseek-coder-7b`     | N√£o                   |
| **Fireworks**     | V√°rios modelos                      | Sim                   |
| **Google Gemini** | `gemini-1.5-pro`                   | Sim                   |
| **Google Vertex AI** | `text-bison@001`                   | Sim                   |
| **Groq**          | `llama3-8b`, `mixtral`              | Sim                   |
| **Hugging Face**  | `code-llama`, `MiniLM`, etc.        | N√£o                   |
| **Mistral AI**    | `mistral-7b`, `mixtral`             | Sim                   |
| **Ollama**        | `llama3`, `mistral`, `gemma`        | N√£o (execu√ß√£o local)  |
| **OpenAI**        | `gpt-4`, `gpt-3.5`                  | Sim                   |
| **Together AI**   | `GPT-JT-6B-v1`                      | Sim                   |
| **Vertex AI**     | `text-bison@001`                    | Sim                   |

A interface de busca √© acess√≠vel por meio da classe `Search`, que realiza busca sem√¢ntica usando embeddings e gera respostas conscientes do contexto.

*[https://v03.api.js.langchain.com/index.html](https://v03.api.js.langchain.com/index.html)*

### Configura√ß√£o

As configura√ß√µes do LLM podem ser ajustadas no arquivo `.cmmv.config.cjs`. Abaixo est√° um exemplo de configura√ß√£o para usar o modelo Gemini 1.5 Pro da Google:

```javascript
module.exports = {
    ai: {
        llm: {
            provider: "google",  // Op√ß√µes: "openai", "deepseek", "huggingface", "gemini", "ollama", "groq"
            model: "gemini-1.5-pro", // Modelo padr√£o para o provedor selecionado
            embeddingTopk: 10, // N√∫mero de resultados top-k usados para recupera√ß√£o de contexto
            textMaxTokens: 2048, // M√°ximo de tokens por resposta
            apiKey: process.env.GOOGLE_API_KEY, // Chave de API para o provedor selecionado (se necess√°rio)
            language: 'pt-br' // Idioma padr√£o da resposta
        }
    }
}
```
<br/>

| **Caminho**          | **Descri√ß√£o**                              | **Valor Padr√£o / Exemplo**                          |
|-------------------|--------------------------------------------|----------------------------------------------------|
| `llm.provider`   | Provedor de LLM a ser usado                | `"google"` (`"openai"`, `"ollama"`, `"huggingface"`, `"groq"`) |
| `llm.model`      | Modelo LLM usado para respostas            | `"gemini-1.5-pro"` (`"gpt-4"`, `"deepseek-coder-7b"`) |
| `llm.embeddingTopk` | N√∫mero de embeddings relevantes a recuperar | `10`                                               |
| `llm.textMaxTokens` | M√°ximo de tokens por requisi√ß√£o           | `2048`                                             |
| `llm.apiKey`     | Chave de API para acessar o provedor de LLM | `process.env.GOOGLE_API_KEY` (se necess√°rio)       |
| `llm.language`   | Idioma padr√£o para respostas               | `"pt-br"` (`"en"`, `"es"`, etc.)                   |

### Consultas de Busca

O exemplo a seguir demonstra como usar o LLM para gerar respostas com base em dados vetoriais recuperados.

```typescript
import { Application, Hook, HooksType } from '@cmmv/core';

import {
    PromptTemplate,
    RunnableSequence,
    RunnablePassthrough,
    StringOutputParser,
    Embedding,
    Dataset,
    Search,
} from '@cmmv/ai';

class SearchSample {
    @Hook(HooksType.onInitialize)
    async start() {
        const question = 'Como criar um controlador CMMV?';

        const search = new Search();
        await search.initialize();

        const finalResult = await search.invoke(question);
        console.log(`Resposta do LLM: `, finalResult.content);
    }
}

Application.exec({
    services: [SearchSample],
})
```

### Como Funciona
<br/>

* **Recupera√ß√£o de Busca**: O sistema de busca recupera os embeddings mais relevantes do banco de dados vetorial.
* **Inje√ß√£o de Contexto**: Os dados recuperados s√£o inseridos no prompt.
* **Processamento do LLM**: O modelo configurado gera uma resposta com base no contexto fornecido.
* **Sa√≠da em JSON**: O resultado √© estruturado em formato JSON para integra√ß√£o cont√≠nua.

Essa configura√ß√£o permite respostas precisas e orientadas por contexto, mantendo a flexibilidade na sele√ß√£o e configura√ß√£o do modelo. üöÄ
