# AI

O m√≥dulo **@cmmv/ai** fornece suporte total para **Retrieval-Augmented Generation (RAG)** e, no futuro, **Retrieval-Augmented Synthesis (RAS)** para **LLMs**. Ele permite a compreens√£o e gera√ß√£o de c√≥digo, suportando:

- **Embeddings**: Integra√ß√£o com modelos de **Hugging Face**, **LLaMA** e mais.
- **Vector Databases**: Suporte nativo para **Qdrant**, **Neo4j** e outros.
- **LLM Interfaces**: Compatibilidade com **Gemini**, **LLaMA**, **ChatGPT** e mais.
- **LangChain Community**: Constru√≠do em `@langchain/community` para alavancar utilit√°rios de IA est√°veis.

## Instalar Python

Antes de instalar o HuggingFace CLI, certifique-se de que **Python** esteja instalado no seu sistema.

Execute o seguinte comando para instalar o Python no Ubuntu:

```sh
$ sudo apt update && sudo apt install python3 python3-pip -y
```

Para outros sistemas operacionais, consulte a [p√°gina de download oficial do Python](https://www.python.org/downloads/).

## Instalar HuggingFace CLI

Depois que o Python estiver instalado, instale o Hugging Face CLI usando **pip**:

```sh
$ pip3 install -U "huggingface_hub[cli]"
```

## CLI seja reconhecida

Se o seu terminal n√£o reconhecer `huggingface-cli`, adicione `~/.local/bin` ao seu **PATH** do sistema:

```sh
$ echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
$ source ~/.bashrc
```

Execute o seguinte comando para verificar a instala√ß√£o:

```sh
$ huggingface-cli --help
```

Se o comando funcionar, a instala√ß√£o foi bem-sucedida! üéâ

## Autentique com HuggingFace

Para acessar e baixar modelos, voc√™ precisa autenticar.

Execute:

```sh
$ huggingface-cli login
```

Voc√™ ser√° solicitado a inserir seu **token de acesso do Hugging Face**.
Gere um em: [Tokens do Hugging Face](https://huggingface.co/settings/tokens)
Certifique-se de que o token tenha permiss√µes de **LEITURA**.

## Baixando modelos

Para baixar um modelo, use o seguinte comando:

```sh
$ huggingface-cli download meta-llama/CodeLlama-7B-Python-hf --local-dir ./models/CodeLlama-7B
```

Isso far√° o download do modelo **CodeLlama 7B Python** no diret√≥rio `./models/CodeLlama-7B`.

Para **CMMV**, defina o caminho do modelo em `.cmmv.config.cjs`:

```js
huggingface: {
    token: process.env.HUGGINGFACE_HUB_TOKEN,
    localModelPath: './models',
    allowRemoteModels: false
},
search: {
    embeddingTopk: 10,
    codeModel: "./models/CodeLlama-7B",
    codeMaxTokens: 512,
    textModel: "google-bert/bert-base-uncased",
    textMaxTokens: 4000,
    baseCodeQuestion: ""
}
```

Agora seu ambiente est√° configurado para usar modelos Hugging Face com **CMMV**! üöÄ

## **Configura√ß√£o**

Dada a complexidade deste m√≥dulo, v√°rias configura√ß√µes s√£o necess√°rias, especialmente para definir quais modelos usar em cada est√°gio. Abaixo est√° a **configura√ß√£o base** com todas as propriedades dispon√≠veis atualmente.

### **Exemplo de configura√ß√£o (`.cmmv.config.cjs`)**
<br/>

```javascript
require('dotenv').config();

module.exports = {
    env: process.env.NODE_ENV,

    ai: {
        huggingface: {
            token: process.env.HUGGINGFACE_HUB_TOKEN,
            localModelPath: './models',
            allowRemoteModels: true
        },
        tokenizer: {
            provider: "huggingface",
            model: "sentence-transformers/distilbert-base-nli-mean-tokens",
            indexSize: 768,
            useKeyBERT: false,
            chunkSize: 500,
            chunkOverlap: 100,
            patterns: [
                '../cmmv/**/*.ts',
                '../cmmv/src/**/*.ts',
                '../cmmv/packages/**/*.ts',
                '../cmmv-*/**/*.ts',
                '../cmmv-*/src/*.ts',
                '../cmmv-*/src/**/*.ts',
                '../cmmv-*/packages/**/*.ts',
                '../cmmv-docs/docs/en/**/*.md'
            ],
            output: "./samples/data.bin",
            ignore: [
                "node_modules", "*.d.ts", "*.cjs",
                "*.spec.ts", "*.test.ts", "/tools/gulp/"
            ],
            exclude: [
                "cmmv-formbuilder", "cmmv-ui",
                "cmmv-language-tools", "cmmv-vue",
                "cmmv-reactivity", "cmmv-vite-plugin",
                "eslint.config.ts", "vitest.config.ts",
                "auto-imports.d.ts", ".d.ts", ".cjs",
                ".spec.ts", ".test.ts", "/tools/gulp/",
                "node_modules"
            ]
        },
        vector: {
            provider: "qdrant",
            qdrant: {
                url: 'http://localhost:6333',
                collection: 'embeddings'
            }
        },
        search: {
            embeddingTopk: 10,
            useCodeModel: true,
            codeModel: "Xenova/codegen-350M-mono",
            codeMaxTokens: 328,
            textModel: "google-bert/bert-base-uncased",
            textMaxTokens: 4000,
            baseCodeQuestion: ""
        }
    }
};
```

## Convertendo modelos

Alguns **LLMs (Large Language Models)** n√£o s√£o nativamente compat√≠veis com todas as estruturas de infer√™ncia. Um exemplo importante √© o **Gemma do Google**, que n√£o √© diretamente suportado por muitas ferramentas. Para usar esses modelos de forma eficiente, voc√™ precisa **convert√™-los para o formato ONNX**.

ONNX (**Open Neural Network Exchange**) √© um formato aberto que otimiza modelos para infer√™ncia eficiente em v√°rias plataformas. Muitas estruturas de infer√™ncia, como **ONNX Runtime**, **TensorRT** e **OpenVINO**, oferecem suporte ao ONNX para uma implanta√ß√£o mais r√°pida e escal√°vel.

Antes de converter, instale os pacotes necess√°rios:

```sh
$ pip3 install -U "optimum[exporters]" onnx onnxruntime
```

Para converter **o modelo Gemma 2B do Google**, execute:

```sh
$ python3 -m optimal.exporters.onnx --model google/gemma-2b ./models/gemma-2b-onnx
```

## Tokenizer

A classe Tokenizer √© respons√°vel por processar e indexar arquivos no pipeline RAG. Ela fornece v√°rias configura√ß√µes, incluindo:

* Padr√µes de pesquisa de arquivo (`patterns`): define padr√µes glob para pesquisar arquivos.
* Exclus√µes e arquivos ignorados (`ignore`, `exclude`): filtra arquivos desnecess√°rios da indexa√ß√£o.
* Modelo de incorpora√ß√£o (`model`): seleciona o modelo Hugging Face para gerar incorpora√ß√µes.
* Tamanho do √≠ndice (`indexSize`): ajusta as dimens√µes de incorpora√ß√£o com base no modelo selecionado.
* Fragmenta√ß√£o (`chunkSize`, `chunkOverlap`): controla a segmenta√ß√£o de dados para melhor recupera√ß√£o.
* Gera√ß√£o de metadados KeyBERT (`useKeyBERT`): extrai palavras-chave para aprimorar pesquisas de vetores.

Para outros modelos que exigem autentica√ß√£o (por exemplo, `LLaMA`), forne√ßa a chave de API em embraceface.token.

### Exemplo de uso:

```typescript
import {Aplicativo, Gancho, TipoDeGanchos} de '@cmmv/core';
import {AIModule} de '@cmmv/ai';

classe TokenizerSample {
    @Hook(HooksType.onInitialize)
    async start() {
        const { Tokenizer } = await import('@cmmv/ai');
        const tokenizer = new Tokenizer();
        tokenizer.start();
    }
}

Aplicativo.exec({
    m√≥dulos: [ModuleAIM],
    servi√ßos: [SampleTokenizer]
});
```
<br/>

1. **Verifica diret√≥rios de projeto** com base na configura√ß√£o `patterns`.
2. **Analisa arquivos TypeScript/JavaScript/Markdown**, extraindo **fun√ß√µes, classes, enums, interfaces, constantes e decoradores**.
3. **Gera embeddings** usando modelos Hugging Face.
4. **Armazena o conjunto de dados** em um arquivo bin√°rio `.bin`.

## Usando KeyBERT

KeyBERT √© um recurso opcional que aprimora a indexa√ß√£o extraindo palavras-chave relevantes. Ele ajuda a refinar os resultados da pesquisa em **FAISS** ou bancos de dados vetoriais, melhorando a precis√£o das **consultas LLM**.

Ao contr√°rio de **TF-IDF**, **YAKE!** ou **RAKE**, que dependem de m√©todos estat√≠sticos, **KeyBERT** aproveita **embeddings BERT** para gerar palavras-chave mais significativas. Isso resulta em melhor filtragem de pesquisa, levando a **respostas baseadas em LLM** mais precisas.

Se o KeyBERT **n√£o estiver habilitado**, o m√©todo de extra√ß√£o de palavra-chave padr√£o ser√° **TF-IDF**, que pode n√£o ser t√£o preciso, mas √© significativamente mais r√°pido.

Antes de usar o KeyBERT, certifique-se de ter o **Python 3** instalado. Em seguida, instale o KeyBERT usando **pip**:

```bash
$ pip install keybert
```

Uma vez instalado, o KeyBERT ser√° usado **durante a tokeniza√ß√£o** para gerar **palavras-chave de filtragem**. Essas palavras-chave melhoram a classifica√ß√£o do conte√∫do indexado, tornando os resultados de pesquisa baseados em vetores mais relevantes.

Se voc√™ preferir **processamento mais r√°pido**, voc√™ pode desabilitar o KeyBERT, e o sistema retornar√° ao **TF-IDF**.

Para habilitar o **KeyBERT**, atualize seu arquivo `.cmmv.config.cjs`:

```javascript
module.exports = {
    ai: {
        tokenizer: {
            useKeyBERT: true // Set to false to use TF-IDF instead
        }
    }
};
```

Com **KeyBERT habilitado**, a filtragem de pesquisa se torna mais **contextual**, levando a **respostas LLM mais precisas**.

Para mais detalhes sobre KeyBERT, visite: [Documenta√ß√£o KeyBERT](https://github.com/MaartenGr/KeyBERT).

## Dataset - FAISS & Vector Storage

A classe **Dataset** gerencia **armazenamento vetorizado** para recupera√ß√£o r√°pida.

* Salva embeddings em **formato bin√°rio** (`.bin`).
* Busca baseada em **FAISS** na mem√≥ria.
* Suporte futuro para **Neo4j, Qdrant, Pinecone**.

```typescript
const dataset = new Dataset();
dataset.save(); // Salva o conjunto de dados em formato bin√°rio
dataset.load(); // Carrega o conjunto de dados na mem√≥ria
```

Para armazenar e pesquisar **embeddings** de forma eficiente, `@cmmv/ai` suporta **Qdrant, Milvus e Neo4j**.

Para executar esses bancos de dados localmente, use os seguintes
**comandos do Docker**:

### **üîπ Qdrant**
```bash
$ docker run -p 6333:6333 --name qdrant-server qdrant/qdrant
```
<br/>
- Executa um servidor **Qdrant** na porta `6333`.
- API dispon√≠vel em `http://localhost:6333`.

### **üîπ Milvus**
```bash
$ docker run -p 19530:19530 --name milvus-server milvusdb/milvus
```
<br/>
- Executa **Milvus** na porta `19530`.
- Requer **Python/Node SDK** para intera√ß√£o.

### **üîπ Neo4j**
```bash
$ docker run --publish=7474:7474 --publish=7687:7687 --volume=$HOME/neo4j/data:/data --name neo4j-server neo4j
```
<br/>
- Executa **Neo4j** nas portas `7474` (HTTP) e `7687` (Bolt).
- Os dados s√£o armazenados persistentemente em `$HOME/neo4j/data`.

# Integra√ß√£o futura

O pr√≥ximo passo √© integrar **modelos pr√©-treinados** para **entendimento e gera√ß√£o de c√≥digo** usando o conjunto de dados tokenizado.

- [x] Tokeniza√ß√£o de **fun√ß√µes, classes, interfaces, decoradores**.
- [x] **Busca de vetores baseada em FAISS** para recupera√ß√£o na mem√≥ria.
- [x] Integra√ß√£o com KeyBert para gera√ß√£o de palavras-chave.
- [ ] Externalizar servidor HTTP para comunica√ß√£o com APIs
- [ ] Integra√ß√£o com **Qdrant, Milvus, Neo4j**.
- [ ] Usar **DeepSeek Code** para **gera√ß√£o de c√≥digo com tecnologia LLM**.
- [ ] Integrar APIs LLM externas como ChatGPT, Gemini, etc.
